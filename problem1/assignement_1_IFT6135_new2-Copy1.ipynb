{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comment to get non-deterministic results\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Several functions in this class implementation are inspired from the NN implemented in cours IFT6093\n",
    "\n",
    "class NN(object):\n",
    "    \n",
    "    '''\n",
    "    Arguments:\n",
    "        input_dim: The input dimension\n",
    "        output_dim: The output dimension\n",
    "        hidden_dims: (h1 dimension, h2 dimension)\n",
    "        n_hidden: number of hidden layers\n",
    "        initialization: type of weigth initialization (zeros, normal or glorot)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim,hidden_dims=(1024,2048),n_hidden=2, initialization='zeros', mode=',train',\n",
    "                 datapath=None,model_path=None):\n",
    "        \n",
    "        self.indim = input_dim\n",
    "        self.hd1 = hidden_dims[0] \n",
    "        self.hd2 = hidden_dims[1]\n",
    "        self.n_hidden = n_hidden\n",
    "        self.outd = output_dim\n",
    "        self.W1 = np.zeros(shape=(hidden_dims[0], input_dim))\n",
    "        self.b1 = np.zeros(hidden_dims[0])\n",
    "        self.W2 = np.zeros(shape=(hidden_dims[1], hidden_dims[0]))\n",
    "        self.b2 = np.zeros(hidden_dims[1])\n",
    "        self.W3 = np.zeros(shape=(output_dim, hidden_dims[1]))\n",
    "        self.b3 = np.zeros(output_dim)\n",
    "        \n",
    "        \n",
    "        if initialization=='normal':\n",
    "            self.initialize_weights_normal()\n",
    "            \n",
    "        if initialization=='glorot':\n",
    "            self.initialize_weights_glorot()\n",
    "           \n",
    "        self.parameters = [self.W3, self.b3, self.W2, self.b2, self.W1, self.b1]\n",
    "        \n",
    "        \n",
    "    def initialize_weights_normal(self):\n",
    "        \n",
    "        self.W1 = np.random.normal(size=(self.hd1, self.indim))\n",
    "        self.W2 = np.random.normal(size=(self.hd2, self.hd1))\n",
    "        self.W3 = np.random.normal(size=(self.outd, self.hd2))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    \n",
    "    def initialize_weights_glorot(self):\n",
    "        \n",
    "        dl1 = np.sqrt(6/(self.indim + self.hd1))\n",
    "        dl2 = np.sqrt(6/(self.hd1 + self.hd2))\n",
    "        dl3 = np.sqrt(6/(self.hd2 + self.outd))\n",
    "        self.W1 = np.random.uniform(low=(-dl1), high=dl1, size=(self.hd1, self.indim))\n",
    "        self.W2 = np.random.uniform(low=(-dl2), high=dl2, size=(self.hd2, self.hd1))\n",
    "        self.W3 = np.random.uniform(low=(-dl3), high=dl3, size=(self.outd, self.hd2))\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    #Method from NN implemented in cours IFT6093\n",
    "    def activation (self,input):\n",
    "        \n",
    "        return (input > 0) * input  \n",
    "    \n",
    "    #line 85\n",
    "\n",
    "    def forward(self,x):\n",
    "                \n",
    "        a1 = np.dot (self.W1, x) + self.b1 \n",
    "        h1 = self.activation (a1)\n",
    "        a2 = np.dot (self.W2, h1) + self.b2\n",
    "        h2 = self.activation (a2)\n",
    "        oa = np.dot (self.W3, h2) + self.b3\n",
    "        os = self.softmax (oa, axis=0)\n",
    "        \n",
    "        return a1, h1, a2, h2, oa, os\n",
    "    \n",
    "    \n",
    "\n",
    "    #Loss ans softmax methods from NN implemented in cours IFT6093\n",
    "    def loss (self, y, os):\n",
    "        return (y * (-np.log(os))).sum()\n",
    "    \n",
    "\n",
    "    def softmax (self,x,axis=1):\n",
    "        shiftx = x - np.max (x, axis=axis, keepdims=True)\n",
    "        exps = np.exp (shiftx)\n",
    "        y = exps / exps.sum (axis=axis, keepdims=True)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def backward(self, x, y, a1, h1, a2, h2, oa, os, weight_decay=0, cache=None):\n",
    "        \n",
    "        grad_oa = os - y\n",
    "        grad_W3 = np.outer (grad_oa, h2) + weight_decay * self.W3\n",
    "        grad_b3 = grad_oa\n",
    "        grad_h2 = np.dot (self.W3.T, grad_oa)\n",
    "        grad_a2 = (a2 > 0) * grad_h2\n",
    "        grad_W2 = np.outer (grad_a2, h1) + weight_decay * self.W2\n",
    "        grad_b2 = grad_a2\n",
    "        grad_h1 = np.dot (self.W2.T, grad_a2)\n",
    "        grad_a1 = (a1 > 0) * grad_h1\n",
    "        grad_W1 = np.outer (grad_a1, x) + weight_decay * self.W1\n",
    "        grad_b1 = grad_a1\n",
    "        grads=[grad_W3, grad_b3, grad_W2, grad_b2, grad_W1, grad_b1]\n",
    "   \n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, grads, learning_rate):\n",
    "        for p, grad in zip(self.parameters, grads):\n",
    "            p -= learning_rate * grad\n",
    "        \n",
    "    #line 201   \n",
    "\n",
    "    def train_SGD(self, x, y_onehot, n, learning_rate=1e-1, weight_decay=0):\n",
    "                \n",
    "        y = y_onehot\n",
    "        losses = 0\n",
    "        if (n==1):\n",
    "            a1, h1, a2, h2, oa, os = self.forward(x)\n",
    "            grads = self.backward(x, y, a1, h1, a2, h2, oa, os)\n",
    "            self.update(grads, learning_rate)\n",
    "            loss = self.loss(y, os)\n",
    "            losses += loss  \n",
    "            average_loss = losses / n\n",
    "        else:    \n",
    "            for j in range(x.shape[0]):\n",
    "                a1, h1, a2, h2, oa, os = self.forward(x[j])\n",
    "                grads = self.backward(x[j], y[j], a1, h1, a2, h2, oa, os)\n",
    "                self.update(grads, learning_rate)\n",
    "                loss = self.loss(y[j], os)\n",
    "                losses += loss     \n",
    "            average_loss = losses / n\n",
    "                \n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    def prediction_SGD (self, x):\n",
    "        predictions = np.zeros(x.shape[0])\n",
    "        for i in range(x.shape[0]):\n",
    "            _, _, _, _, _, os = self.forward(x[i])\n",
    "            predictions[i] = os.argmax()\n",
    "                \n",
    "        return predictions\n",
    "                \n",
    "    \n",
    "    def accuracy_SGD (self, prediction, y):\n",
    "        accuracies=0\n",
    "        for i in range (y.shape[0]):\n",
    "            accuracies+=(prediction[i]==y[i])\n",
    "            \n",
    "        return accuracies / y.shape[0]\n",
    "    \n",
    "    \n",
    "    def test_SGD(self, x, y_onehot, y):\n",
    "        pred=np.zeros(y.shape[0])\n",
    "        avg_loss=0\n",
    "        for i in range (x.shape[0]):\n",
    "            _, _, _, _, _, os = self.forward(x[i])\n",
    "            loss=self.loss (y_onehot[i], os)\n",
    "            avg_loss+=loss\n",
    "            pred[i]=os.argmax()\n",
    "            \n",
    "        accuracy=self.accuracy_SGD(pred, y)    \n",
    "        return avg_loss / x.shape[0] , accuracy\n",
    "    \n",
    "   \n",
    "    def forward_mbatch(self, x):\n",
    "                \n",
    "        a1 = np.dot ( x, self.W1.T) + self.b1\n",
    "        h1 = self.activation (a1)\n",
    "        a2 = np.dot (h1, self.W2.T) + self.b2\n",
    "        h2 = self.activation (a2)\n",
    "        oa = np.dot (h2, self.W3.T) + self.b3\n",
    "        os = self.softmax (oa, axis=1)\n",
    "                \n",
    "        return a1, h1, a2, h2, oa, os\n",
    "    \n",
    "    #line 303\n",
    "        \n",
    "    def backward_mbatch(self, x, y, a1, h1, a2, h2, oa, os, batch_n, weight_decay=0):\n",
    "                \n",
    "        batch_n = x.shape[0]\n",
    "        bgrad_oa = os - y\n",
    "        bgrad_W3 = np.dot (bgrad_oa.T, h2) / batch_n  + weight_decay * self.W3\n",
    "        bgrad_b3 = bgrad_oa.mean(axis=0)\n",
    "        bgrad_h2 = np.dot (bgrad_oa, self.W3)\n",
    "        bgrad_a2 = (a2 > 0) * bgrad_h2\n",
    "        bgrad_W2 = np.dot (bgrad_a2.T, h1) / batch_n  + weight_decay * self.W2\n",
    "        bgrad_b2 = bgrad_a2.mean(axis=0)\n",
    "        bgrad_h1 = np.dot (bgrad_a2, self.W2)\n",
    "        bgrad_a1 = (a1 > 0) * bgrad_h1\n",
    "        bgrad_W1 = np.dot (bgrad_a1.T, x) / batch_n  + weight_decay * self.W1\n",
    "        bgrad_b1 = bgrad_a1.mean(axis=0)\n",
    "        bgrads=[bgrad_W3, bgrad_b3, bgrad_W2, bgrad_b2, bgrad_W1, bgrad_b1]\n",
    "   \n",
    "        return bgrads\n",
    "\n",
    "    #line 360\n",
    "\n",
    "    #Method taken fron homwork 3 in cours IFT6093\n",
    "    def loss_mbatch(self, os, y):\n",
    "        return (y * (-np.log(os))).sum(axis=1).mean(axis=0)     \n",
    "        \n",
    "    \n",
    "    #training with minibatch gradient decent\n",
    "    def train_mbatch(self, x, y_onehot, mb_size=100, learning_rate=1e-1, weight_decay=0):\n",
    "                \n",
    "        average_loss=0\n",
    "        for i in range (0, x.shape[0], mb_size):\n",
    "            xi = x[i:(i+mb_size)]\n",
    "            yi = y_onehot[i:(i+mb_size)]\n",
    "        \n",
    "            #losses = 0\n",
    "            a1, h1, a2, h2, oa, os = self.forward_mbatch(xi)\n",
    "            grads = self.backward_mbatch (xi, yi,a1, h1, a2, h2,oa, os, mb_size)\n",
    "            self.update(grads, learning_rate)\n",
    "            average_loss = self.loss_mbatch(os, yi) \n",
    "                          \n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    #line 385\n",
    "    \n",
    "    def prediction_mbatch (self, x):\n",
    "        _, _, _, _, _, os = self.forward_mbatch(x)\n",
    "        return os.argmax(axis=1)\n",
    "    \n",
    "\n",
    "    def accuracy_mbatch (self, prediction, y):\n",
    "        accuracy = np.zeros(y.shape[0])\n",
    "        accuracy = prediction == y\n",
    "        return accuracy.mean(axis=0)\n",
    "    \n",
    "\n",
    "    def test_mbatch(self, x, y_onehot, y):\n",
    "        _, _, _, _, _, os = self.forward_mbatch(x)\n",
    "        loss = self.loss_mbatch (os, y_onehot)\n",
    "        accuracy=self.accuracy_mbatch (os.argmax(axis=1), y)\n",
    "        return loss, accuracy\n",
    "    \n",
    "    \n",
    "    def finite_difference():\n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function taken from IFT6093 cours\n",
    "def onehot(y, n_classes):\n",
    "    o = np.zeros(shape=(y.shape[0], n_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        o[i, int(y[i])] = 1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#part of code inspired from : https://stackoverflow.com/questions/40427435/extract-images-from-idx3-ubyte-file-or-gzip-via-python\n",
    "\n",
    "import gzip\n",
    "def dataloader_X(filename, rows, cols, image_size = 0):\n",
    "    f = gzip.open(filename,'r')\n",
    "    image_size = image_size\n",
    "    num_images = rows\n",
    "\n",
    "    f.read(8)\n",
    "    buf = f.read(image_size * image_size * num_images)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float)\n",
    "    c=data.reshape(rows, image_size, image_size)\n",
    "    print(c.shape)\n",
    "    c=data.reshape(rows, (image_size*image_size) )\n",
    "    print(c.shape)  \n",
    "        \n",
    "    return c\n",
    "\n",
    "def dataloader_y(filename, rows):\n",
    "    f = gzip.open(filename,'r')\n",
    "\n",
    "    f.read(8)\n",
    "    buf = f.read()\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.float)\n",
    "    print(labels.shape)  \n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Charging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n",
      "(10000, 28, 28)\n",
      "(10000, 784)\n",
      "(60000,)\n",
      "(10000,)\n",
      "X_train shape =  (55000, 784)\n",
      "y_train shape =  (55000,)\n",
      "X_valid shape =  (5000, 784)\n",
      "y_valid shape =  (5000,)\n",
      "X_test shape =  (10000, 784)\n",
      "y_test shape =  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "X_train = dataloader_X('train-images-idx3-ubyte.gz', 60000, 284, image_size = 28 )\n",
    "X_test = dataloader_X('t10k-images-idx3-ubyte.gz', 10000, 284, image_size = 28 )\n",
    "y_train = dataloader_y('train-labels-idx1-ubyte.gz', 60000)\n",
    "y_test =  dataloader_y('t10k-labels-idx1-ubyte.gz', 10000)\n",
    "\n",
    "\n",
    "# Data normalization\n",
    "X_train= X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "\n",
    "# Data randomization\n",
    "indices = list(range(len(X_train)))\n",
    "shuffle(indices)\n",
    "\n",
    "#Split train set in training and validation set\n",
    "X_valid, y_valid = X_train[indices[55000:]], y_train[indices[55000:]]\n",
    "X_train, y_train = X_train[indices[:55000]], y_train[indices[:55000]]\n",
    "\n",
    "digit_y_train_onehot= onehot (y_train, 10)\n",
    "digit_y_valid_onehot= onehot (y_valid, 10)\n",
    "digit_y_test_onehot= onehot (y_test, 10)\n",
    "\n",
    "\n",
    "print('X_train shape = ', X_train.shape)\n",
    "print('y_train shape = ', y_train.shape)\n",
    "print('X_valid shape = ', X_valid.shape)\n",
    "print('y_valid shape = ', y_valid.shape)\n",
    "print('X_test shape = ', X_test.shape)\n",
    "print('y_test shape = ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weigths initialization, data from dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:205: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:205: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with minibatch gradient decent implementation: 849.515589 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAFyCAYAAABlU6npAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ad4fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1. Weigths initialization data from dataloader\n",
    "\n",
    "print('Weigths initialization, data from dataloader')\n",
    "\n",
    "# Set timer\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "#training 10 epochs for each initialization type\n",
    "epochs=10\n",
    "\n",
    "zeros_losses=[]\n",
    "normal_losses=[]\n",
    "glorot_losses=[]\n",
    "weigths=['zeros', 'normal', 'glorot']\n",
    "\n",
    "for i, init in enumerate(weigths, 0):\n",
    "    \n",
    "    NN_digits= NN(784, 10, hidden_dims=(500,300), initialization=init)\n",
    "\n",
    "    for epoch in range (epochs): \n",
    "    \n",
    "        loss=NN_digits.train_mbatch(X_train, digit_y_train_onehot, mb_size=100)\n",
    "        if (init=='zeros'):\n",
    "            zeros_losses.append(loss)\n",
    "        if (init == 'normal'):\n",
    "            normal_losses.append(loss)\n",
    "        if (init == 'glorot'):\n",
    "            glorot_losses.append(loss)\n",
    "            \n",
    "time_mb = time.time() - start_time\n",
    "print('Time with minibatch gradient decent implementation: %f seconds\\n' % time_mb)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs),  zeros_losses, label='zeros')\n",
    "plt.plot(range(epochs), normal_losses, label='normal')\n",
    "plt.plot(range(epochs), glorot_losses, label='glorot')\n",
    "plt.title(\"Initialization\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions declaration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model using different hyperparameters, like mini-batch size, learning rate and epochs number\n",
    "# x_ds is the dataset to train, y_ds is the target dataset.\n",
    "\n",
    "\n",
    "# function to test different hyperparameters\n",
    "# x_ds and y_ds must be the training set\n",
    "\n",
    "\n",
    "\n",
    "def hyperparameter_search(model, x_t, y_t, y_t_o, x_v, y_v, y_v_o, epochs, minibatch, learningrate,\n",
    "                          verbose=False):\n",
    "    \n",
    "    # NN model\n",
    "    # NN_mbatch_1 = NN(784, 10, hidden_dims=(500,300), initialization='glorot')\n",
    "    \n",
    "    # Model training \n",
    "    losses_train, accs_train = [], []\n",
    "    losses_valid, accs_valid = [], []\n",
    "    #losses_test, preds_test = [], []\n",
    "        \n",
    "        \n",
    "    for epoch in range (epochs): \n",
    "        \n",
    "        # Always train in training set\n",
    "                \n",
    "        loss_mbatch_1 = model.train_mbatch(x_t, y_t_o, mb_size=minibatch,\n",
    "                                                 learning_rate=learningrate)\n",
    "        \n",
    "        if verbose:\n",
    "            print('epoch ', epoch, ' loss ', loss_mbatch_1)\n",
    "            \n",
    "            \n",
    "        #Test in validation set to adjust Hyperparameters        \n",
    "        loss_train, acc_train = model.test_mbatch(x_t, y_t_o, y_t)\n",
    "        loss_valid, acc_valid = model.test_mbatch(x_v, y_v_o, y_v)        \n",
    "        \n",
    "        \n",
    "        losses_train.append(loss_train) \n",
    "        accs_train.append(acc_train)\n",
    "        losses_valid.append(loss_valid)\n",
    "        accs_valid.append(acc_valid)\n",
    "        \n",
    "    \n",
    "    #Test in validation set to adjust Hyperparameters\n",
    "    \n",
    "    #loss,accuracy = NN_mbatch_1.test_mbatch(x_ds,y,y_ds)\n",
    "    print('epoch ', epoch, ' loss ', loss_mbatch_1)\n",
    "    \n",
    "    #Only at the end, test in test set \n",
    "    \n",
    "    return losses_train, acc_train, losses_valid, acc_valid, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "epoch  0  loss  0.14581453092038102\n",
      "epoch  1  loss  0.08653862662573525\n",
      "epoch  2  loss  0.05584058661953494\n",
      "epoch  3  loss  0.03977169112867238\n",
      "epoch  4  loss  0.029163141280530488\n",
      "epoch  5  loss  0.022867748552135483\n",
      "epoch  6  loss  0.018304303721175932\n",
      "epoch  7  loss  0.015072465969688517\n",
      "epoch  8  loss  0.012517681531048827\n",
      "epoch  9  loss  0.01040159495356941\n",
      "epoch  9  loss  0.01040159495356941\n",
      "600\n",
      "epoch  0  loss  0.14400304299939368\n",
      "epoch  1  loss  0.08327159855685291\n",
      "epoch  2  loss  0.05222551711269508\n",
      "epoch  3  loss  0.036253690260122685\n",
      "epoch  4  loss  0.027346689746163687\n",
      "epoch  5  loss  0.022180972603945564\n",
      "epoch  6  loss  0.018563148203478017\n",
      "epoch  7  loss  0.015449373732080315\n",
      "epoch  8  loss  0.013088962738955691\n",
      "epoch  9  loss  0.010971804726865384\n",
      "epoch  9  loss  0.010971804726865384\n",
      "800\n",
      "epoch  0  loss  0.1239439884161044\n",
      "epoch  1  loss  0.06904779049486472\n",
      "epoch  2  loss  0.04559550672097166\n",
      "epoch  3  loss  0.03323956798016456\n",
      "epoch  4  loss  0.026215202329884675\n",
      "epoch  5  loss  0.02090813018692287\n",
      "epoch  6  loss  0.017183363502691805\n",
      "epoch  7  loss  0.014092723809152037\n",
      "epoch  8  loss  0.011661536928523992\n",
      "epoch  9  loss  0.009474041347659282\n",
      "epoch  9  loss  0.009474041347659282\n",
      "1000\n",
      "epoch  0  loss  0.13386080325897712\n",
      "epoch  1  loss  0.06853407031124321\n",
      "epoch  2  loss  0.04347046397321121\n",
      "epoch  3  loss  0.030546175092571914\n",
      "epoch  4  loss  0.022513852737911573\n",
      "epoch  5  loss  0.017469377744004512\n",
      "epoch  6  loss  0.013928021921748912\n",
      "epoch  7  loss  0.010831812208381229\n",
      "epoch  8  loss  0.008784875503456939\n",
      "epoch  9  loss  0.0070813779376932805\n",
      "epoch  9  loss  0.0070813779376932805\n"
     ]
    }
   ],
   "source": [
    "# testing for different h1 size\n",
    "\n",
    "\n",
    "\n",
    "for i, h1 in enumerate([500, 600, 800, 1000]):\n",
    "    print(h1)\n",
    "    \n",
    "    NN_mbatch_1 = NN(784, 10, hidden_dims=(h1,300), initialization='glorot')\n",
    "    \n",
    "    losses_train, preds_train, losses_valid, preds_valid, NN_mbatch_1 = hyperparameter_search(\n",
    "        NN_mbatch_1, X_train, y_train, digit_y_train_onehot, X_valid, y_valid, digit_y_valid_onehot, \n",
    "        10, 100, 1e-1, verbose=True)\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Approximation of the gradient of the loos at the end of training, with respect to W3 \n",
    "# (the second layer weigths) with to the first p = min(10;m) elements of W3.\n",
    "\n",
    "\n",
    "#function to calculate the finite difference for  \n",
    "\n",
    "def loop_finite_diff(self, x, y, epsilon=1e-5):\n",
    "        a1, h1, a2, h2, oa, os = self.forward(x)\n",
    "        grads = self.backward(x, y, a1, h1, a2, h2, oa, os)\n",
    "        loss = self.loss(y, os)\n",
    "        \n",
    "        grads_finite_diff = []\n",
    "        \n",
    "        for p in self.parameters[0]:\n",
    "            grad_fdiff = np.zeros(shape=p.shape)\n",
    "            for i, v in np.ndenumerate(p):\n",
    "                p[i] += epsilon\n",
    "                _, _, _, _, _, os = self.forward(x)\n",
    "                loss_diff = self.loss(os, y)\n",
    "                grad_fdiff[index] = (loss_diff - loss) / epsilon\n",
    "                p[index] -= epsilon\n",
    "            grads_finite_diff.append(grad_fdiff)\n",
    "        return gradients_finite_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 1000, 100000, 50, 5000]\n",
      "[0.1, 0.001, 1e-05, 0.02, 0.0002]\n"
     ]
    }
   ],
   "source": [
    "#1. epsilon = 1 / N\n",
    "\n",
    "#Use at least 5 values of N from the set {k10**i : i E {0; : : : ; 5g} k E {1, 5}}\n",
    "epsilon=[]\n",
    "N = []\n",
    "for exp in range (1, 6, 2):\n",
    "    a= 10**exp\n",
    "    N.append(a)\n",
    "for exp in range (1, 5, 2):\n",
    "    b= 5*10**exp\n",
    "    N.append(b)\n",
    "for i in range (5):\n",
    "    epsilon.append(1/N[i])\n",
    "\n",
    "print (N)\n",
    "print (epsilon)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
