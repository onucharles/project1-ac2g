{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comment to get non-deterministic results\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Several functions in this class implementation are inspired from the NN implemented in cours IFT6093\n",
    "\n",
    "class NN(object):\n",
    "    \n",
    "    '''\n",
    "    Arguments:\n",
    "        input_dim: The input dimension\n",
    "        output_dim: The output dimension\n",
    "        hidden_dims: (h1 dimension, h2 dimension)\n",
    "        n_hidden: number of hidden layers\n",
    "        initialization: type of weigth initialization (zeros, normal or glorot)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim,hidden_dims=(1024,2048),n_hidden=2, initialization='zeros', mode=',train',\n",
    "                 datapath=None,model_path=None):\n",
    "        \n",
    "        self.indim = input_dim\n",
    "        self.hd1 = hidden_dims[0] \n",
    "        self.hd2 = hidden_dims[1]\n",
    "        self.n_hidden = n_hidden\n",
    "        self.outd = output_dim\n",
    "        self.W1 = np.zeros(shape=(hidden_dims[0], input_dim))\n",
    "        self.b1 = np.zeros(hidden_dims[0])\n",
    "        self.W2 = np.zeros(shape=(hidden_dims[1], hidden_dims[0]))\n",
    "        self.b2 = np.zeros(hidden_dims[1])\n",
    "        self.W3 = np.zeros(shape=(output_dim, hidden_dims[1]))\n",
    "        self.b3 = np.zeros(output_dim)\n",
    "        \n",
    "        \n",
    "        if initialization=='normal':\n",
    "            self.initialize_weights_normal()\n",
    "            \n",
    "        if initialization=='glorot':\n",
    "            self.initialize_weights_glorot()\n",
    "           \n",
    "        self.parameters = [self.W3, self.b3, self.W2, self.b2, self.W1, self.b1]\n",
    "        \n",
    "        \n",
    "    def initialize_weights_normal(self):\n",
    "        \n",
    "        self.W1 = np.random.normal(size=(self.hd1, self.indim))\n",
    "        self.W2 = np.random.normal(size=(self.hd2, self.hd1))\n",
    "        self.W3 = np.random.normal(size=(self.outd, self.hd2))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    \n",
    "    def initialize_weights_glorot(self):\n",
    "        \n",
    "        dl1 = np.sqrt(6/(self.indim + self.hd1))\n",
    "        dl2 = np.sqrt(6/(self.hd1 + self.hd2))\n",
    "        dl3 = np.sqrt(6/(self.hd2 + self.outd))\n",
    "        self.W1 = np.random.uniform(low=(-dl1), high=dl1, size=(self.hd1, self.indim))\n",
    "        self.W2 = np.random.uniform(low=(-dl2), high=dl2, size=(self.hd2, self.hd1))\n",
    "        self.W3 = np.random.uniform(low=(-dl3), high=dl3, size=(self.outd, self.hd2))\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    #Method from NN implemented in cours IFT6093\n",
    "    def activation (self,input):\n",
    "        \n",
    "        return (input > 0) * input  \n",
    "    \n",
    "    #line 85\n",
    "\n",
    "    def forward(self,x):\n",
    "                \n",
    "        a1 = np.dot (self.W1, x) + self.b1 \n",
    "        h1 = self.activation (a1)\n",
    "        a2 = np.dot (self.W2, h1) + self.b2\n",
    "        h2 = self.activation (a2)\n",
    "        oa = np.dot (self.W3, h2) + self.b3\n",
    "        os = self.softmax (oa, axis=0)\n",
    "        \n",
    "        return a1, h1, a2, h2, oa, os\n",
    "    \n",
    "    \n",
    "\n",
    "    #Loss ans softmax methods from NN implemented in cours IFT6093\n",
    "    def loss (self, y, os):\n",
    "        return (y * (-np.log(os))).sum()\n",
    "    \n",
    "\n",
    "    def softmax (self,x,axis=1):\n",
    "        shiftx = x - np.max (x, axis=axis, keepdims=True)\n",
    "        exps = np.exp (shiftx)\n",
    "        y = exps / exps.sum (axis=axis, keepdims=True)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def backward(self, x, y, a1, h1, a2, h2, oa, os, weight_decay=0, cache=None):\n",
    "        \n",
    "        grad_oa = os - y\n",
    "        grad_W3 = np.outer (grad_oa, h2) + weight_decay * self.W3\n",
    "        grad_b3 = grad_oa\n",
    "        grad_h2 = np.dot (self.W3.T, grad_oa)\n",
    "        grad_a2 = (a2 > 0) * grad_h2\n",
    "        grad_W2 = np.outer (grad_a2, h1) + weight_decay * self.W2\n",
    "        grad_b2 = grad_a2\n",
    "        grad_h1 = np.dot (self.W2.T, grad_a2)\n",
    "        grad_a1 = (a1 > 0) * grad_h1\n",
    "        grad_W1 = np.outer (grad_a1, x) + weight_decay * self.W1\n",
    "        grad_b1 = grad_a1\n",
    "        grads=[grad_W3, grad_b3, grad_W2, grad_b2, grad_W1, grad_b1]\n",
    "   \n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, grads, learning_rate):\n",
    "        for p, grad in zip(self.parameters, grads):\n",
    "            p -= learning_rate * grad\n",
    "        \n",
    "    #line 201   \n",
    "\n",
    "    def train_SGD(self, x, y_onehot, n, learning_rate=1e-1, weight_decay=0):\n",
    "                \n",
    "        y = y_onehot\n",
    "        losses = 0\n",
    "        if (n==1):\n",
    "            a1, h1, a2, h2, oa, os = self.forward(x)\n",
    "            grads = self.backward(x, y, a1, h1, a2, h2, oa, os)\n",
    "            self.update(grads, learning_rate)\n",
    "            loss = self.loss(y, os)\n",
    "            losses += loss  \n",
    "            average_loss = losses / n\n",
    "        else:    \n",
    "            for j in range(x.shape[0]):\n",
    "                a1, h1, a2, h2, oa, os = self.forward(x[j])\n",
    "                grads = self.backward(x[j], y[j], a1, h1, a2, h2, oa, os)\n",
    "                self.update(grads, learning_rate)\n",
    "                loss = self.loss(y[j], os)\n",
    "                losses += loss     \n",
    "            average_loss = losses / n\n",
    "                \n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    def prediction_SGD (self, x):\n",
    "        predictions = np.zeros(x.shape[0])\n",
    "        for i in range(x.shape[0]):\n",
    "            _, _, _, _, _, os = self.forward(x[i])\n",
    "            predictions[i] = os.argmax()\n",
    "                \n",
    "        return predictions\n",
    "                \n",
    "    \n",
    "    def accuracy_SGD (self, prediction, y):\n",
    "        accuracies=0\n",
    "        for i in range (y.shape[0]):\n",
    "            accuracies+=(prediction[i]==y[i])\n",
    "            \n",
    "        return accuracies / y.shape[0]\n",
    "    \n",
    "    \n",
    "    def test_SGD(self, x, y_onehot, y):\n",
    "        pred=np.zeros(y.shape[0])\n",
    "        avg_loss=0\n",
    "        for i in range (x.shape[0]):\n",
    "            _, _, _, _, _, os = self.forward(x[i])\n",
    "            loss=self.loss (y_onehot[i], os)\n",
    "            avg_loss+=loss\n",
    "            pred[i]=os.argmax()\n",
    "            \n",
    "        accuracy=self.accuracy_SGD(pred, y)    \n",
    "        return avg_loss / x.shape[0] , accuracy\n",
    "    \n",
    "   \n",
    "    def forward_mbatch(self, x):\n",
    "                \n",
    "        a1 = np.dot ( x, self.W1.T) + self.b1\n",
    "        h1 = self.activation (a1)\n",
    "        a2 = np.dot (h1, self.W2.T) + self.b2\n",
    "        h2 = self.activation (a2)\n",
    "        oa = np.dot (h2, self.W3.T) + self.b3\n",
    "        os = self.softmax (oa, axis=1)\n",
    "                \n",
    "        return a1, h1, a2, h2, oa, os\n",
    "    \n",
    "    #line 303\n",
    "        \n",
    "    def backward_mbatch(self, x, y, a1, h1, a2, h2, oa, os, batch_n, weight_decay=0):\n",
    "                \n",
    "        batch_n = x.shape[0]\n",
    "        bgrad_oa = os - y\n",
    "        bgrad_W3 = np.dot (bgrad_oa.T, h2) / batch_n  + weight_decay * self.W3\n",
    "        bgrad_b3 = bgrad_oa.mean(axis=0)\n",
    "        bgrad_h2 = np.dot (bgrad_oa, self.W3)\n",
    "        bgrad_a2 = (a2 > 0) * bgrad_h2\n",
    "        bgrad_W2 = np.dot (bgrad_a2.T, h1) / batch_n  + weight_decay * self.W2\n",
    "        bgrad_b2 = bgrad_a2.mean(axis=0)\n",
    "        bgrad_h1 = np.dot (bgrad_a2, self.W2)\n",
    "        bgrad_a1 = (a1 > 0) * bgrad_h1\n",
    "        bgrad_W1 = np.dot (bgrad_a1.T, x) / batch_n  + weight_decay * self.W1\n",
    "        bgrad_b1 = bgrad_a1.mean(axis=0)\n",
    "        bgrads=[bgrad_W3, bgrad_b3, bgrad_W2, bgrad_b2, bgrad_W1, bgrad_b1]\n",
    "   \n",
    "        return bgrads\n",
    "\n",
    "    #line 360\n",
    "\n",
    "    #Method taken fron homwork 3 in cours IFT6093\n",
    "    def loss_mbatch(self, os, y):\n",
    "        return (y * (-np.log(os))).sum(axis=1).mean(axis=0)     \n",
    "        \n",
    "    \n",
    "    #training with minibatch gradient decent\n",
    "    def train_mbatch(self, x, y_onehot, mb_size=100, learning_rate=1e-1, weight_decay=0):\n",
    "                \n",
    "        average_loss=0\n",
    "        for i in range (0, x.shape[0], mb_size):\n",
    "            xi = x[i:(i+mb_size)]\n",
    "            yi = y_onehot[i:(i+mb_size)]\n",
    "        \n",
    "            #losses = 0\n",
    "            a1, h1, a2, h2, oa, os = self.forward_mbatch(xi)\n",
    "            grads = self.backward_mbatch (xi, yi,a1, h1, a2, h2,oa, os, mb_size)\n",
    "            self.update(grads, learning_rate)\n",
    "            average_loss = self.loss_mbatch(os, yi) \n",
    "                          \n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    #line 385\n",
    "    \n",
    "    def prediction_mbatch (self, x):\n",
    "        _, _, _, _, _, os = self.forward_mbatch(x)\n",
    "        return os.argmax(axis=1)\n",
    "    \n",
    "\n",
    "    def accuracy_mbatch (self, prediction, y):\n",
    "        accuracy = np.zeros(y.shape[0])\n",
    "        accuracy = prediction == y\n",
    "        return accuracy.mean(axis=0)\n",
    "    \n",
    "\n",
    "    def test_mbatch(self, x, y_onehot, y):\n",
    "        _, _, _, _, _, os = self.forward_mbatch(x)\n",
    "        loss = self.loss_mbatch (os, y_onehot)\n",
    "        accuracy=self.accuracy_mbatch (os.argmax(axis=1), y)\n",
    "        return loss, accuracy\n",
    "    \n",
    "    \n",
    "    def finite_difference():\n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function taken from IFT6093 cours\n",
    "def onehot(y, n_classes):\n",
    "    o = np.zeros(shape=(y.shape[0], n_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        o[i, int(y[i])] = 1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#part of code inspired from : https://stackoverflow.com/questions/40427435/extract-images-from-idx3-ubyte-file-or-gzip-via-python\n",
    "\n",
    "import gzip\n",
    "def dataloader_X(filename, rows, cols, image_size = 0):\n",
    "    f = gzip.open(filename,'r')\n",
    "    image_size = image_size\n",
    "    num_images = rows\n",
    "\n",
    "    f.read(8)\n",
    "    buf = f.read(image_size * image_size * num_images)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float)\n",
    "    c=data.reshape(rows, image_size, image_size)\n",
    "    print(c.shape)\n",
    "    c=data.reshape(rows, (image_size*image_size) )\n",
    "    print(c.shape)  \n",
    "        \n",
    "    return c\n",
    "\n",
    "def dataloader_y(filename, rows):\n",
    "    f = gzip.open(filename,'r')\n",
    "\n",
    "    f.read(8)\n",
    "    buf = f.read()\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.float)\n",
    "    print(labels.shape)  \n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Charging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n",
      "(10000, 28, 28)\n",
      "(10000, 784)\n",
      "(60000,)\n",
      "(10000,)\n",
      "X_train shape =  (55000, 784)\n",
      "y_train shape =  (55000,)\n",
      "X_valid shape =  (5000, 784)\n",
      "y_valid shape =  (5000,)\n",
      "X_test shape =  (10000, 784)\n",
      "y_test shape =  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "X_train = dataloader_X('train-images-idx3-ubyte.gz', 60000, 284, image_size = 28 )\n",
    "X_test = dataloader_X('t10k-images-idx3-ubyte.gz', 10000, 284, image_size = 28 )\n",
    "y_train = dataloader_y('train-labels-idx1-ubyte.gz', 60000)\n",
    "y_test =  dataloader_y('t10k-labels-idx1-ubyte.gz', 10000)\n",
    "\n",
    "\n",
    "#data normalization\n",
    "X_train= X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "\n",
    "# Data randomization\n",
    "indices = list(range(len(X_train)))\n",
    "shuffle(indices)\n",
    "\n",
    "X_valid, y_valid = X_train[indices[55000:]], y_train[indices[55000:]]\n",
    "X_train, y_train = X_train[indices[:55000]], y_train[indices[:55000]]\n",
    "\n",
    "digit_y_train_onehot= onehot (y_train, 10)\n",
    "digit_y_valid_onehot= onehot (y_valid, 10)\n",
    "digit_y_test_onehot= onehot (y_test, 10)\n",
    "\n",
    "\n",
    "print('X_train shape = ', X_train.shape)\n",
    "print('y_train shape = ', y_train.shape)\n",
    "print('X_valid shape = ', X_valid.shape)\n",
    "print('y_valid shape = ', y_valid.shape)\n",
    "print('X_test shape = ', X_test.shape)\n",
    "print('y_test shape = ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weigths initialization data from dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:373: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:373: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with minibatch gradient decent implementation: 867.412985 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAFyCAYAAABlU6npAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ4EkSQIMBZIkKff/AdEKEnrNxCEGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x868fe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1. Weigths initialization data from dataloader\n",
    "\n",
    "print('Weigths initialization, data from dataloader')\n",
    "\n",
    "import time\n",
    "\n",
    "#training 10 epochs\n",
    "start_time = time.time()\n",
    "\n",
    "epochs=10\n",
    "\n",
    "zeros_losses=[]\n",
    "normal_losses=[]\n",
    "glorot_losses=[]\n",
    "\n",
    "weigths=['zeros', 'normal', 'glorot']\n",
    "\n",
    "for i, init in enumerate(weigths, 0):\n",
    "    \n",
    "    NN_digits= NN(784, 10, hidden_dims=(500,300), initialization=init)\n",
    "\n",
    "    for epoch in range (epochs): \n",
    "    \n",
    "        loss=NN_digits.train_mbatch(X_train, digit_y_train_onehot, mb_size=100)\n",
    "        if (init=='zeros'):\n",
    "            zeros_losses.append(loss)\n",
    "        if (init == 'normal'):\n",
    "            normal_losses.append(loss)\n",
    "        if (init == 'glorot'):\n",
    "            glorot_losses.append(loss)\n",
    "            \n",
    "time_mb = time.time() - start_time\n",
    "print('Time with minibatch gradient decent implementation: %f seconds\\n' % time_mb)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs),  zeros_losses, label='zeros')\n",
    "plt.plot(range(epochs), normal_losses, label='normal')\n",
    "plt.plot(range(epochs), glorot_losses, label='glorot')\n",
    "plt.title(\"Initialization\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finite difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Approximation of the gradient of the loos at the end of training, with respect to W3 \n",
    "# (the second layer weigths) with to the first p = min(10;m) elements of W3.\n",
    "\n",
    "\n",
    "#function to calculate the finite difference for  \n",
    "\n",
    "def loop_finite_diff(self, x, y, epsilon=1e-5):\n",
    "        a1, h1, a2, h2, oa, os = self.forward(x)\n",
    "        grads = self.backward(x, y, a1, h1, a2, h2, oa, os)\n",
    "        loss = self.loss(y, os)\n",
    "        \n",
    "        grads_finite_diff = []\n",
    "        \n",
    "        for p in self.parameters[0]:\n",
    "            grad_fdiff = np.zeros(shape=p.shape)\n",
    "            for i, v in np.ndenumerate(p):\n",
    "                p[i] += epsilon\n",
    "                _, _, _, _, _, os = self.forward(x)\n",
    "                loss_diff = self.loss(os, y)\n",
    "                grad_fdiff[index] = (loss_diff - loss) / epsilon\n",
    "                p[index] -= epsilon\n",
    "            grads_finite_diff.append(grad_fdiff)\n",
    "        return gradients_finite_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 1000, 100000, 50, 5000]\n",
      "[0.1, 0.001, 1e-05, 0.02, 0.0002]\n"
     ]
    }
   ],
   "source": [
    "#1. epsilon = 1 / N\n",
    "\n",
    "#Use at least 5 values of N from the set {k10**i : i E {0; : : : ; 5g} k E {1, 5}}\n",
    "epsilon=[]\n",
    "N = []\n",
    "for exp in range (1, 6, 2):\n",
    "    a= 10**exp\n",
    "    N.append(a)\n",
    "for exp in range (1, 5, 2):\n",
    "    b= 5*10**exp\n",
    "    N.append(b)\n",
    "for i in range (5):\n",
    "    epsilon.append(1/N[i])\n",
    "\n",
    "print (N)\n",
    "print (epsilon)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
