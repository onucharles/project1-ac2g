{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comment to get non-deterministic results\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This class implementation is inspired from the NN implemented in cours IFT6093\n",
    "class NN(object):\n",
    "    \n",
    "    '''\n",
    "    @Arguments:\n",
    "        input_dim: The input dimension\n",
    "        output_dim: The output dimension\n",
    "        hidden_dims: (h1 dimension, h2 dimension)\n",
    "        n_hidden: number of hidden layers\n",
    "        initialization: type of weigth initialization (zeros, normal or glorot)\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim,hidden_dims=(1024,2048),n_hidden=2, initialization='zeros', mode=',train',\n",
    "                 datapath=None,model_path=None):\n",
    "        \n",
    "        self.indim = input_dim\n",
    "        self.hd1 = hidden_dims[0] \n",
    "        self.hd2 = hidden_dims[1]\n",
    "        self.n_hidden = n_hidden\n",
    "        self.outd = output_dim\n",
    "        self.W1 = np.zeros(shape=(hidden_dims[0], input_dim))\n",
    "        #print('W1.shape =', self.W1.shape)\n",
    "        #print('W1 = ', self.W1)\n",
    "        #print('\\n')\n",
    "        \n",
    "        self.b1 = np.zeros(hidden_dims[0])\n",
    "        #print('b1.shape =', self.b1.shape)\n",
    "        #print('b1 = ', self.b1)\n",
    "        #print('\\n')\n",
    "        \n",
    "        self.W2 = np.zeros(shape=(hidden_dims[1], hidden_dims[0]))\n",
    "        #print('W2.shape =', self.W2.shape)\n",
    "        #print('W2 = ', self.W2)\n",
    "        #print('\\n')\n",
    "        \n",
    "        self.b2 = np.zeros(hidden_dims[1])\n",
    "        #print('b2.shape =', self.b2.shape)\n",
    "        #print('b2 = ', self.b2)\n",
    "        #print('\\n')\n",
    "        \n",
    "        self.W3 = np.zeros(shape=(output_dim, hidden_dims[1]))\n",
    "        #print('W3.shape =', self.W3.shape)\n",
    "        #print('W3 = ', self.W3)\n",
    "        #print('\\n')\n",
    "        \n",
    "        self.b3 = np.zeros(output_dim)\n",
    "        #print('b3.shape =', self.b3.shape)\n",
    "        #print('b3 = ', self.b3)\n",
    "        #print('\\n')\n",
    "        \n",
    "        if initialization=='normal':\n",
    "            self.initialize_weights_normal()\n",
    "            #print('W1 = ', self.W1)\n",
    "            #print('W2 = ', self.W2)\n",
    "            #print('W3 = ', self.W3)\n",
    "            \n",
    "            \n",
    "        if initialization=='glorot':\n",
    "            self.initialize_weights_glorot()\n",
    "            #print('W1 = ', self.W1)\n",
    "            #print('W2 = ', self.W2)\n",
    "            #print('W3 = ', self.W3)\n",
    "            \n",
    "        \n",
    "        self.parameters = [self.W3, self.b3, self.W2, self.b2, self.W1, self.b1]\n",
    "        \n",
    "        \n",
    "    def initialize_weights_normal(self):\n",
    "        \n",
    "        self.W1 = np.random.normal(size=(self.hd1, self.indim))\n",
    "        self.W2 = np.random.normal(size=(self.hd2, self.hd1))\n",
    "        self.W3 = np.random.normal(size=(self.outd, self.hd2))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def initialize_weights_glorot(self):\n",
    "        \n",
    "        dl1 = np.sqrt(6/(self.indim + self.hd1))\n",
    "        dl2 = np.sqrt(6/(self.hd1 + self.hd2))\n",
    "        dl3 = np.sqrt(6/(self.hd2 + self.outd))\n",
    "        self.W1 = np.random.uniform(low=(-dl1), high=dl1, size=(self.hd1, self.indim))\n",
    "        self.W2 = np.random.uniform(low=(-dl2), high=dl2, size=(self.hd2, self.hd1))\n",
    "        self.W3 = np.random.uniform(low=(-dl3), high=dl3, size=(self.outd, self.hd2))\n",
    "        \n",
    "        return self\n",
    "        \n",
    "        \n",
    "        \n",
    "    #Method inspired from NN implemented in cours IFT6093\n",
    "    def activation (self,input):\n",
    "        return (input > 0) * input  \n",
    "    \n",
    "    #line 85\n",
    "\n",
    "    def forward(self,x):\n",
    "        #print('forward')\n",
    "        \n",
    "        a1 = np.dot (self.W1, x) + self.b1 \n",
    "        #print('a1 = np.dot (self.W1, x) + self.b1')\n",
    "        #print('a1.shape =', a1.shape)\n",
    "        #print('a1 = ', a1)\n",
    "        #print('\\n')\n",
    "        \n",
    "        h1 = self.activation (a1)\n",
    "        #print('h1 = self.activation (a1)')\n",
    "        #print('h1.shape =', h1.shape)\n",
    "        #print('h1 = ', h1)\n",
    "        #print('\\n')\n",
    "        \n",
    "        a2 = np.dot (self.W2, h1) + self.b2\n",
    "        #print('a2 = np.dot (self.W2, h1) + self.b2')\n",
    "        #print('a2.shape =', a2.shape)\n",
    "        #print('a2 = ', a2)\n",
    "        #print('\\n')\n",
    "        \n",
    "        h2 = self.activation (a2)\n",
    "        #print('h2 = self.activation (a2)')\n",
    "        #print('h2.shape =', h2.shape)\n",
    "        #print('h2 = ', h2)\n",
    "        #print('\\n')\n",
    "        \n",
    "    \n",
    "        oa = np.dot (self.W3, h2) + self.b3\n",
    "        #print('oa = np.dot (self.W3, h2) + self.b3')\n",
    "        #print('oa.shape =', oa.shape)\n",
    "        #print('oa = ', oa)\n",
    "        #print('\\n')\n",
    "        \n",
    "        os = self.softmax (oa, axis=0)\n",
    "        #print('os = softmax (oa)')\n",
    "        #print('os.shape =', os.shape)\n",
    "        #print('os = ', os)\n",
    "        #print('\\n')\n",
    "               \n",
    "        return a1, h1, a2, h2, oa, os\n",
    "    \n",
    "    \n",
    "\n",
    "    #Methods inspired from NN implemented in cours IFT6093\n",
    "    def loss (self, y, os):\n",
    "        return (y * (-np.log(os))).sum()\n",
    "    \n",
    "\n",
    "    def softmax (self,x,axis=1):\n",
    "        shiftx = x - np.max (x, axis=axis, keepdims=True)\n",
    "        exps = np.exp (shiftx)\n",
    "        y = exps / exps.sum (axis=axis, keepdims=True)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def backward(self, x, y, a1, h1, a2, h2, oa, os, weight_decay=0, cache=None):\n",
    "        #print ('backward')\n",
    "        #print('x.shape = ', x.shape)\n",
    "        #print('y.shape = ', y.shape)\n",
    "        #print('os.shape = ', os.shape)\n",
    "        grad_oa = os - y\n",
    "        #print('grad_oa.shape =', grad_oa.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_W3 = np.outer (grad_oa, h2) + weight_decay * self.W3\n",
    "        #print('grad_W3.shape =', grad_W3.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_b3 = grad_oa\n",
    "        #print('grad_b3.shape =', grad_b3.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_h2 = np.dot (self.W3.T, grad_oa)\n",
    "        #print(' grad_h2.shape =', grad_h2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_a2 = (a2 > 0) * grad_h2\n",
    "        #print('grad_a2.shape =', grad_a2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_W2 = np.outer (grad_a2, h1) + weight_decay * self.W2\n",
    "        #print('grad_W2.shape =', grad_W2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_b2 = grad_a2 \n",
    "        #print('grad_b2.shape =', grad_b2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_h1 = np.dot (self.W2.T, grad_a2)\n",
    "        #print('grad_h1.shape =', grad_h1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_a1 = (a1 > 0) * grad_h1\n",
    "        #print('grad_a1.shape =', grad_a1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_W1 = np.outer (grad_a1, x) + weight_decay * self.W1\n",
    "        #print('grad_W1.shape =', grad_W1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_b1 = grad_a1\n",
    "        #print('grad_b1.shape =', grad_b1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grads=[grad_W3, grad_b3, grad_W2, grad_b2, grad_W1, grad_b1]\n",
    "   \n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, grads, learning_rate):\n",
    "        for p, grad in zip(self.parameters, grads):\n",
    "            p -= learning_rate * grad\n",
    "        \n",
    "    #line 201   \n",
    "\n",
    "    def train_SGD(self, x, y_onehot, n, learning_rate=1e-1, weight_decay=0):\n",
    "        y= y_onehot\n",
    "        #print('x.shape = ', x.shape)\n",
    "        #print('y.shape = ', y.shape)\n",
    "        losses = 0\n",
    "        if (n==1):\n",
    "            a1, h1, a2, h2, oa, os = self.forward(x)\n",
    "            grads = self.backward(x, y, a1, h1, a2, h2, oa, os)\n",
    "            self.update(grads, learning_rate)\n",
    "            loss = self.loss(y, os)\n",
    "            losses += loss  \n",
    "            average_loss = losses / n\n",
    "        else:    \n",
    "            for j in range(x.shape[0]):\n",
    "                a1, h1, a2, h2, oa, os = self.forward(x[j])\n",
    "                grads = self.backward(x[j], y[j], a1, h1, a2, h2, oa, os)\n",
    "                self.update(grads, learning_rate)\n",
    "                loss = self.loss(y[j], os)\n",
    "                losses += loss \n",
    "                \n",
    "            average_loss = losses / n\n",
    "            #print (average_loss)\n",
    "\n",
    "        #print (average_loss)   \n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    def prediction_SGD (self, x):\n",
    "        predictions = np.zeros(x.shape[0])\n",
    "        for i in range(x.shape[0]):\n",
    "            _, _, _, _, _, os = self.forward(x[i])\n",
    "            predictions[i] = os.argmax()\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def accuracy_SGD (self, prediction, y):\n",
    "        accuracies=0\n",
    "        for i in range (y.shape[0]):\n",
    "            accuracies+=(prediction[i]==y[i])\n",
    "            \n",
    "        return accuracies / y.shape[0]\n",
    "    \n",
    "    \n",
    "    def test_SGD(self, x, y_onehot, y):\n",
    "        pred=np.zeros(y.shape[0])\n",
    "        avg_loss=0\n",
    "        for i in range (x.shape[0]):\n",
    "            _, _, _, _, _, os = self.forward(x[i])\n",
    "            loss=self.loss (y_onehot[i], os)\n",
    "            avg_loss+=loss\n",
    "            pred[i]=os.argmax()\n",
    "            \n",
    "        accuracy=self.accuracy_SGD(pred, y)    \n",
    "        return avg_loss / x.shape[0] , accuracy\n",
    "    \n",
    "   \n",
    "    def forward_mbatch(self, x):\n",
    "        #print ('forward minibtach')\n",
    "        a1 = np.dot ( x, self.W1.T) + self.b1 \n",
    "        #print('a1 = np.dot (x, self.W1.T) + self.b1')\n",
    "        #print('a1.shape =', a1.shape)\n",
    "        #print('a1 = ', a1)\n",
    "        #print('\\n')\n",
    "        \n",
    "        h1 = self.activation (a1)\n",
    "        #print('h1 = self.activation (a1)')\n",
    "        #print('h1.shape =', h1.shape)\n",
    "        #print('h1 = ', h1)\n",
    "        #print('\\n')\n",
    "        \n",
    "        a2 = np.dot (h1, self.W2.T) + self.b2\n",
    "        #print('a2 = np.dot (h1, self.W2.T) + self.b2')\n",
    "        #print('a2.shape =', a2.shape)\n",
    "        #print('a2 = ', a2)\n",
    "        #print('\\n')\n",
    "        \n",
    "        h2 = self.activation (a2)\n",
    "        #print('h2 = self.activation (a2)')\n",
    "        #print('h2.shape =', h2.shape)\n",
    "        #print('h2 = ', h2)\n",
    "        #print('\\n')\n",
    "        \n",
    "        oa = np.dot (h2, self.W3.T) + self.b3\n",
    "        #print('oa = np.dot (h2, self.W3.T) + self.b3')\n",
    "        #print('oa.shape =', oa.shape)\n",
    "        #print('oa = ', oa)\n",
    "        #print('\\n')\n",
    "        \n",
    "        os = self.softmax (oa, axis=1)\n",
    "        #print('os = softmax (oa)')\n",
    "        #print('os.shape =', os.shape)\n",
    "        #print('os = ', os)\n",
    "        #print('\\n')\n",
    "               \n",
    "        return a1, h1, a2, h2, oa, os\n",
    "    \n",
    "    #line 303\n",
    "        \n",
    "    def backward_mbatch(self, x, y, a1, h1, a2, h2, oa, os, batch_n, weight_decay=0):\n",
    "        #print ('backward minibatch')\n",
    "        \n",
    "        #print('x.shape = ', x.shape)\n",
    "        #print('y.shape = ', y.shape)\n",
    "        #print('os.shape = ', os.shape)\n",
    "        \n",
    "        \n",
    "        batch_n = x.shape[0]\n",
    "        bgrad_oa = os - y\n",
    "        #print('bgrad_oa.shape =', bgrad_oa.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_W3 = np.dot (bgrad_oa.T, h2) / batch_n  + weight_decay * self.W3\n",
    "        #print('bgrad_W3.shape =', bgrad_W3.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_b3 = bgrad_oa.mean(axis=0)\n",
    "        #print('bgrad_b3.shape =', bgrad_b3.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_h2 = np.dot (bgrad_oa, self.W3)\n",
    "        #print(' bgrad_h2.shape =', bgrad_h2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_a2 = (a2 > 0) * bgrad_h2\n",
    "        #print('bgrad_a2.shape =', bgrad_a2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_W2 = np.dot (bgrad_a2.T, h1) / batch_n  + weight_decay * self.W2\n",
    "        #print('bgrad_W2.shape =', bgrad_W2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_b2 = bgrad_a2.mean(axis=0) \n",
    "        #print('bgrad_b2.shape =', bgrad_b2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_h1 = np.dot (bgrad_a2, self.W2)\n",
    "        #print('bgrad_h1.shape =', bgrad_h1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "     \n",
    "        bgrad_a1 = (a1 > 0) * bgrad_h1\n",
    "        #print('bgrad_a1.shape =', bgrad_a1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_W1 = np.dot (bgrad_a1.T, x) / batch_n  + weight_decay * self.W1\n",
    "        #print('bgrad_W1.shape =', bgrad_W1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_b1 = bgrad_a1.mean(axis=0)\n",
    "        #print('bgrad_b1.shape =', bgrad_b1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrads=[bgrad_W3, bgrad_b3, bgrad_W2, bgrad_b2, bgrad_W1, bgrad_b1]\n",
    "   \n",
    "        return bgrads\n",
    "\n",
    "    #line 360\n",
    "\n",
    "    #Method taken fron homwork 3 in cours IFT6093\n",
    "    def loss_mbatch(self, os, y):\n",
    "        return (y * (-np.log(os))).sum(axis=1).mean(axis=0)     \n",
    "        \n",
    "    \n",
    "    #training with minibatch gradient decent\n",
    "    def train_mbatch(self, x, y_onehot, mb_size=100, learning_rate=1e-1, weight_decay=0):\n",
    "        average_loss=0\n",
    "        for i in range (0, x.shape[0], mb_size):\n",
    "            #print (i)\n",
    "            xi = x[i:(i+mb_size)]\n",
    "            yi = y_onehot[i:(i+mb_size)]\n",
    "        \n",
    "            losses = 0\n",
    "            a1, h1, a2, h2, oa, os = self.forward_mbatch(xi)\n",
    "            grads = self.backward_mbatch (xi, yi,a1, h1, a2, h2,oa, os, mb_size)\n",
    "            self.update(grads, learning_rate)\n",
    "            average_loss = self.loss_mbatch(os, yi) \n",
    "                          \n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    #line 385\n",
    "    \n",
    "    def prediction_mbatch (self, x):\n",
    "        _, _, _, _, _, os = self.forward_mbatch(x)\n",
    "        return os.argmax(axis=1)\n",
    "    \n",
    "\n",
    "    def accuracy_mbatch (self, prediction, y):\n",
    "        accuracy = np.zeros(y.shape[0])\n",
    "        accuracy = prediction == y\n",
    "        return accuracy.mean(axis=0)\n",
    "    \n",
    "\n",
    "    def test_mbatch(self, x, y_onehot, y):\n",
    "        _, _, _, _, _, os = self.forward_mbatch(x)\n",
    "        loss = self.loss_mbatch (os, y_onehot)\n",
    "        accuracy=self.accuracy_mbatch (os.argmax(axis=1), y)\n",
    "        return loss, accuracy\n",
    "    \n",
    "    \n",
    "    def finite_difference():\n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax (self, x):\n",
    "        shiftx = x - np.max(x)\n",
    "        exps=np.exp(shiftx)\n",
    "        y=exps/np.sum(exps)\n",
    "        return y\n",
    "\n",
    "def relu (x):\n",
    "    y=np.maximum(0, x)\n",
    "    return y\n",
    "\n",
    "#function taken from IFT6093 cours\n",
    "def onehot(y, n_classes):\n",
    "    o = np.zeros(shape=(y.shape[0], n_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        o[i, int(y[i])] = 1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (55000, 784)\n",
      "y_train shape =  (55000,)\n",
      "X_valid shape =  (5000, 784)\n",
      "y_valid shape =  (5000,)\n",
      "X_test shape =  (10000, 784)\n",
      "y_test shape =  (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#loading and preparing data set 2\n",
    "\n",
    "\n",
    "from mnist import MNIST\n",
    "from random import shuffle\n",
    "\n",
    "mndata = MNIST('C:/Users/Geo/Documents/Bioinformatica/maitrise/representationLearning/devoir 1')\n",
    "\n",
    "mndata.gz = True\n",
    "\n",
    "X_train, y_train= mndata.load_training()\n",
    "X_test, y_test=  mndata.load_testing()\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "X_train= X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "indices = list(range(len(X_train)))\n",
    "shuffle(indices)\n",
    "\n",
    "X_valid, y_valid = X_train[indices[55000:]], y_train[indices[55000:]]\n",
    "X_train, y_train = X_train[indices[:55000]], y_train[indices[:55000]]\n",
    "\n",
    "digit_y_train_onehot= onehot (y_train, 10)\n",
    "digit_y_valid_onehot= onehot (y_valid, 10)\n",
    "digit_y_test_onehot= onehot (y_test, 10)\n",
    "\n",
    "\n",
    "print('X_train shape = ', X_train.shape)\n",
    "print('y_train shape = ', y_train.shape)\n",
    "print('X_valid shape = ', X_valid.shape)\n",
    "print('y_valid shape = ', y_valid.shape)\n",
    "print('X_test shape = ', X_test.shape)\n",
    "print('y_test shape = ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:373: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:373: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with minibatch gradient decent implementation: 867.391612 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAFyCAYAAABlU6npAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJMBQIEmScoYCSZIEGAokSVLu/wPPdZoJa4s1rAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7b424e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1. Weigths initialization\n",
    "\n",
    "import time\n",
    "\n",
    "#training 10 epochs\n",
    "start_time = time.time()\n",
    "\n",
    "epochs=10\n",
    "\n",
    "zeros_losses=[]\n",
    "normal_losses=[]\n",
    "glorot_losses=[]\n",
    "\n",
    "weigths=['zeros', 'normal', 'glorot']\n",
    "\n",
    "for i, init in enumerate(weigths, 0):\n",
    "    \n",
    "    # input_dim, output_dim, hidden_dims=(1024,2048), n_hidden=2, initialization='zeros', mode=',train',\n",
    "    # datapath=None,model_path=None\n",
    "\n",
    "    NN_digits= NN(784, 10, hidden_dims=(500,300), initialization=init)\n",
    "\n",
    "    for epoch in range (epochs): \n",
    "    #x, y, mb_size=100, learning_rate=1e-1, weight_decay=0\n",
    "    \n",
    "        loss=NN_digits.train_mbatch(X_train, digit_y_train_onehot, mb_size=100)\n",
    "        \n",
    "        if (init=='zeros'):\n",
    "            zeros_losses.append(loss)\n",
    "        if (init == 'normal'):\n",
    "            normal_losses.append(loss)\n",
    "        if (init == 'glorot'):\n",
    "            glorot_losses.append(loss)\n",
    "            \n",
    "time_mb = time.time() - start_time\n",
    "print('Time with minibatch gradient decent implementation: %f seconds\\n' % time_mb)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs),  zeros_losses, label='zeros')\n",
    "plt.plot(range(epochs), normal_losses, label='normal')\n",
    "plt.plot(range(epochs), glorot_losses, label='glorot')\n",
    "plt.title(\"Initialization\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/40427435/extract-images-from-idx3-ubyte-file-or-gzip-via-python\n",
    "\n",
    "import gzip\n",
    "def dataloader_X(filename, rows, cols, image_size = 0):\n",
    "    f = gzip.open(filename,'r')\n",
    "    image_size = image_size\n",
    "    num_images = rows\n",
    "\n",
    "    f.read(8)\n",
    "    buf = f.read(image_size * image_size * num_images)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float)\n",
    "    c=data.reshape(rows, image_size, image_size)\n",
    "    print(c.shape)\n",
    "    c=data.reshape(rows, (image_size*image_size) )\n",
    "    print(c.shape)  \n",
    "        \n",
    "    return c\n",
    "\n",
    "def dataloader_y(filename, rows):\n",
    "    f = gzip.open(filename,'r')\n",
    "\n",
    "    f.read(8)\n",
    "    buf = f.read()\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.float)\n",
    "    print(labels.shape)  \n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n",
      "(10000, 28, 28)\n",
      "(10000, 784)\n",
      "(60000,)\n",
      "(10000,)\n",
      "X_train shape =  (55000, 784)\n",
      "y_train shape =  (55000,)\n",
      "X_valid shape =  (5000, 784)\n",
      "y_valid shape =  (5000,)\n",
      "X_test shape =  (10000, 784)\n",
      "y_test shape =  (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = dataloader_X('train-images-idx3-ubyte.gz', 60000, 284, image_size = 28 )\n",
    "X_test = dataloader_X('t10k-images-idx3-ubyte.gz', 10000, 284, image_size = 28 )\n",
    "y_train = dataloader_y('train-labels-idx1-ubyte.gz', 60000)\n",
    "y_test =  dataloader_y('t10k-labels-idx1-ubyte.gz', 10000)\n",
    "\n",
    "X_train= X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "indices = list(range(len(X_train)))\n",
    "shuffle(indices)\n",
    "\n",
    "X_valid, y_valid = X_train[indices[55000:]], y_train[indices[55000:]]\n",
    "X_train, y_train = X_train[indices[:55000]], y_train[indices[:55000]]\n",
    "\n",
    "digit_y_train_onehot= onehot (y_train, 10)\n",
    "digit_y_valid_onehot= onehot (y_valid, 10)\n",
    "digit_y_test_onehot= onehot (y_test, 10)\n",
    "\n",
    "\n",
    "print('X_train shape = ', X_train.shape)\n",
    "print('y_train shape = ', y_train.shape)\n",
    "print('X_valid shape = ', X_valid.shape)\n",
    "print('y_valid shape = ', y_valid.shape)\n",
    "print('X_test shape = ', X_test.shape)\n",
    "print('y_test shape = ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weigths initialization data from dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:373: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Miniconda3\\lib\\site-packages\\ipykernel\\__main__.py:373: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with minibatch gradient decent implementation: 867.412985 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAFyCAYAAABlU6npAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ4EkSQIMBZIkKff/AdEKEnrNxCEGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x868fe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1. Weigths initialization data from dataloader\n",
    "\n",
    "print('Weigths initialization data from dataloader')\n",
    "\n",
    "import time\n",
    "\n",
    "#training 10 epochs\n",
    "start_time = time.time()\n",
    "\n",
    "epochs=10\n",
    "\n",
    "zeros_losses=[]\n",
    "normal_losses=[]\n",
    "glorot_losses=[]\n",
    "\n",
    "weigths=['zeros', 'normal', 'glorot']\n",
    "\n",
    "for i, init in enumerate(weigths, 0):\n",
    "    \n",
    "    # input_dim, output_dim, hidden_dims=(1024,2048), n_hidden=2, initialization='zeros', mode=',train',\n",
    "    # datapath=None,model_path=None\n",
    "\n",
    "    NN_digits= NN(784, 10, hidden_dims=(500,300), initialization=init)\n",
    "\n",
    "    for epoch in range (epochs): \n",
    "    #x, y, mb_size=100, learning_rate=1e-1, weight_decay=0\n",
    "    \n",
    "        loss=NN_digits.train_mbatch(X_train, digit_y_train_onehot, mb_size=100)\n",
    "        \n",
    "        if (init=='zeros'):\n",
    "            zeros_losses.append(loss)\n",
    "        if (init == 'normal'):\n",
    "            normal_losses.append(loss)\n",
    "        if (init == 'glorot'):\n",
    "            glorot_losses.append(loss)\n",
    "            \n",
    "time_mb = time.time() - start_time\n",
    "print('Time with minibatch gradient decent implementation: %f seconds\\n' % time_mb)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(epochs),  zeros_losses, label='zeros')\n",
    "plt.plot(range(epochs), normal_losses, label='normal')\n",
    "plt.plot(range(epochs), glorot_losses, label='glorot')\n",
    "plt.title(\"Initialization\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1. Parameter search \n",
    "\n",
    "# input_dim, output_dim,hidden_dims=(1024,2048),n_hidden=2,mode=',train',\n",
    "# datapath=None,model_path=None\n",
    "\n",
    "NN_digits_mbatch= NN(784, 10, hidden_dims=(500,300))\n",
    "\n",
    "#training 10 epochs\n",
    "start_time = time.time()\n",
    "\n",
    "epoc=[]\n",
    "train_losses=[]\n",
    "train_accuracies=[]\n",
    "valid_losses=[]\n",
    "valid_accuracies=[]\n",
    "test_losses_mb=[]\n",
    "test_accuracies_mb=[]\n",
    "\n",
    "epochs=10\n",
    "\n",
    "print('epoch      train loss      train accuracy         test loss         test accuracy  ')\n",
    "\n",
    "weigths=['zeros', 'normal', 'glorot']\n",
    "\n",
    "for i, init in enumerate(weigths, 0):\n",
    "    \n",
    "    print (init)\n",
    "    \n",
    "    # input_dim, output_dim, hidden_dims=(1024,2048), n_hidden=2, initialization='zeros', mode=',train',\n",
    "    # datapath=None,model_path=None\n",
    "\n",
    "    NN_digits= NN(784, 10, hidden_dims=(500,300), initialization=init)\n",
    "\n",
    "    for epoch in range (epochs): \n",
    "    #x, y, mb_size=100, learning_rate=1e-1, weight_decay=0\n",
    "        loss=NN_digits.train_mbatch(X_train, digit_y_train_onehot, mb_size=100)\n",
    "    \n",
    "        loss_train_mb, accuracy_train_mb = NN_digits.test_mbatch(X_train, digit_y_train_onehot, \n",
    "                                                                        y_train)\n",
    "    \n",
    "        loss_valid_mb, accuracy_valid_mb = NN_digits.test_mbatch(X_valid, digit_y_valid_onehot, \n",
    "                                                                        y_valid)\n",
    "    \n",
    "    \n",
    "        loss_test_mb, accuracy_test_mb = NN_digits.test_mbatch(X_test, digit_y_test_onehot,\n",
    "                                                                      y_test)\n",
    "        \n",
    "        print(epoch, '  ', loss_train_mb, '  ', accuracy_train_mb , '  ', loss_test_mb,\n",
    "          '  ', accuracy_test_mb)\n",
    "        \n",
    "        epoc.append(epoch)\n",
    "        train_losses_mb.append(loss_train_mb)\n",
    "        train_accuracies_mb.append(accuracy_train_mb)\n",
    "        valid_losses_mb.append(loss_valid_mb)\n",
    "        valid_accuracies_mb.append(accuracy_valid_mb)\n",
    "        test_losses_mb.append(loss_test_mb)\n",
    "        test_accuracies_mb.append(accuracy_test_mb)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "time_mb = time.time() - start_time\n",
    "\n",
    "print('Time with minibatch gradient decent implementation: %f seconds\\n' % time_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finite difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Approximation of the gradient of the loos at the end of training, with respect to W3 \n",
    "# (the second layer weigths) with to the first p = min(10;m) elements of W3.\n",
    "\n",
    "\n",
    "#function to calculate the finite difference for  \n",
    "\n",
    "def loop_finite_diff(self, x, y, epsilon=1e-5):\n",
    "        a1, h1, a2, h2, oa, os = self.forward(x)\n",
    "        grads = self.backward(x, y, a1, h1, a2, h2, oa, os)\n",
    "        loss = self.loss(y, os)\n",
    "        \n",
    "        grads_finite_diff = []\n",
    "        \n",
    "        for p in self.parameters[0]:\n",
    "            grad_fdiff = np.zeros(shape=p.shape)\n",
    "            for i, v in np.ndenumerate(p):\n",
    "                p[i] += epsilon\n",
    "                _, _, _, _, _, os = self.forward(x)\n",
    "                loss_diff = self.loss(os, y)\n",
    "                grad_fdiff[index] = (loss_diff - loss) / epsilon\n",
    "                p[index] -= epsilon\n",
    "            grads_finite_diff.append(grad_fdiff)\n",
    "        return gradients_finite_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 1000, 100000, 50, 5000]\n",
      "[0.1, 0.001, 1e-05, 0.02, 0.0002]\n"
     ]
    }
   ],
   "source": [
    "#1. epsilon = 1 / N\n",
    "\n",
    "#Use at least 5 values of N from the set {k10**i : i E {0; : : : ; 5g} k E {1, 5}}\n",
    "epsilon=[]\n",
    "N = []\n",
    "for exp in range (1, 6, 2):\n",
    "    a= 10**exp\n",
    "    N.append(a)\n",
    "for exp in range (1, 5, 2):\n",
    "    b= 5*10**exp\n",
    "    N.append(b)\n",
    "for i in range (5):\n",
    "    epsilon.append(1/N[i])\n",
    "\n",
    "print (N)\n",
    "print (epsilon)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoc.append(epoch)\n",
    "train_losses_mb.append(loss_train_mb)\n",
    "train_accuracies_mb.append(accuracy_train_mb)\n",
    "valid_losses_mb.append(loss_valid_mb)\n",
    "valid_accuracies_mb.append(accuracy_valid_mb)\n",
    "test_losses_mb.append(loss_test_mb)\n",
    "test_accuracies_mb.append(accuracy_test_mb)\n",
    "\n",
    "print ()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
