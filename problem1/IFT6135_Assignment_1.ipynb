{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IFT6135-H2019\n",
    "Assignment 1 Programming part\n",
    "Multilayer Parceptrons and Convolutional Neural Networks\n",
    "\n",
    "Alejandra\n",
    "Amlin\n",
    "Charles\n",
    "Georgina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    shiftx = x - np.max(x)\n",
    "    exps=np.exp(shiftx)\n",
    "    y=exps/np.sum(exps)\n",
    "    return y\n",
    "\n",
    "def relu (x):\n",
    "    y=np.maximum(0, x)\n",
    "    return y\n",
    "\n",
    "#function taken from IFT6093 cours\n",
    "def onehot(y, n_classes):\n",
    "    o = np.zeros(shape=(y.shape[0], n_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        o[i, int(y[i])] = 1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This class implementation is inspired from the NN implemented in cours IFT6093\n",
    "class NN(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_dim, n, output_dim,hidden_dims=(1024,2048),n_hidden=2,mode='train',\n",
    "                 datapath=None,model_path=None):\n",
    "        \n",
    "        self.hd1 = hidden_dims[0] \n",
    "        self.hd2 = hidden_dims[1]\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n = n\n",
    "        self.outd = output_dim\n",
    "        self.W1 = np.zeros(size=(hidden_dims[0], input_dim))\n",
    "        self.b1 = np.zeros(hidden_dims[0])\n",
    "        self.W2 = np.zeros(size=(hidden_dims[1], hidden_dims[0]))\n",
    "        self.b2 = np.zeros(hidden_dims[1])\n",
    "        self.W3 = np.zeros(size=(output_dim, hidden_dims[1]))\n",
    "        self.b3 = np.zeros(output_dim)\n",
    "        self.parameters = [self.b1, self.W1, self.b2, self.W2, self.b3, self.W3]\n",
    "        \n",
    "        \n",
    "    def initialize_weights_normal(self,n_hidden,dims):\n",
    "        \n",
    "        self.W1 = np.random.normal(loc=0.0, scale=1.0, size=(self.hd1, self.n))\n",
    "        self.W2 = np.random.normal(loc=0.0, scale=1.0, size=(self.hd2, self.hd1))\n",
    "        self.W3 = np.random.normal(loc=0.0, scale=1.0, size=(self.outd, self.hd2))\n",
    "    \n",
    "    \n",
    "    def initialize_weights_glorot(self,n_hidden,dims):\n",
    "        \n",
    "        dl1 = np.sqrt(6/(self.hd1 + self.hd2))\n",
    "        dl2 = np.sqrt(6/(self.hd2 + self.outd))\n",
    "        self.W1 = np.random.uniform(low=(-dl1), high=dl1, size=(self.hd1, self.n))\n",
    "        self.W2 = np.random.uniform(low=(-dl1), high=dl1, size=(self.hd2, self.hd1))\n",
    "        self.W3 = np.random.uniform(low=(-dl2), high=dl2, size=(self.outd, self.hd2))\n",
    "    \n",
    "\n",
    "    def forward(self,input,labels):\n",
    "        \n",
    "        a1 = np.dot (self.W1, x) + self.b1 \n",
    "        h1 = activation (h_a)\n",
    "        a2 = np.dot (self.W2, x) + self.b2\n",
    "        h2 = activation (a2)\n",
    "        oa = np.dot (self.W3, x) + self.b3\n",
    "        os = softmax (oa, axis=0)\n",
    "               \n",
    "        return a1, h1, a2, h2, oa, os\n",
    "    \n",
    "        \n",
    "\n",
    "    #Method inspired from NN implemented in cours IFT6093\n",
    "    def activation (self,input):\n",
    "        return (input > 0) * input\n",
    "    \n",
    "\n",
    "    #Method inspired from NN implemented in cours IFT6093\n",
    "    def loss (self,prediction,os):\n",
    "        return (y * (-np.log(os))).sum()\n",
    "    \n",
    "\n",
    "    def softmax (self,input,axis=1):\n",
    "        shiftx = x - np.max (x, axis=axis, keepdims=True)\n",
    "        exps = np.exp (shiftx)\n",
    "        y = exps / exps.sum (axis=axis, keepdims=True)\n",
    "        return y\n",
    "    \n",
    "\n",
    "    def backward(self,cache, x, y,a1, h1, a2, h2, oa, os, weight_decay=0):\n",
    "    \n",
    "        grad_oa = os - y\n",
    "        grad_W3 = np.outer (grad_oa, h2) + weight_decay * self.W3\n",
    "        grad_b3 = grad_oa\n",
    "        grad_h2 = np.dot (self.W3.T, grad_oa)\n",
    "        grad_a2 = (a2 > 0) * grad_h2\n",
    "        grad_W2 = np.outer (grad_a2, h1) + weight_decay * self.W2\n",
    "        grad_b2 = grad_a2 \n",
    "        grad_h1 = np.dot (self.W2.T, grad_oa)\n",
    "        grad_a1 = (a1 > 0) * grad_h1\n",
    "        grad_W1 = np.outer (grad_a1, x) + weight_decay * self.W1\n",
    "        grad_b1 = grad_a1\n",
    "   \n",
    "        return grad_W3, grad_b3, grad_W2, grad_b2, grad_W1, grad_b1\n",
    "\n",
    "\n",
    "\n",
    "    def update(self,average_grads, mu):\n",
    "    \n",
    "        for p, grad in zip(self.parameters, average_grads):\n",
    "            p -= mu * grad\n",
    "    \n",
    "        #self.b1= self.b1+mu*grad_b1\n",
    "        #self.W1= self.W1+mu*grad_W1\n",
    "        #self.b2= self.b2+mu*grad_b2\n",
    "        #self.W2= self.W2+mu*grad_W2\n",
    "        #self.b3= self.b3+mu*grad_b3\n",
    "        #self.W3= self.W3+mu*grad_W3\n",
    "        \n",
    "       \n",
    "\n",
    "    def train_SGD(self, x, y, mb_size=100, learning_rate=1e-1, weight_decay=0):\n",
    "        \n",
    "        for i in range (0, x.shape[0], mb_size):\n",
    "        \n",
    "            xi = x[i:(i+mb_size)]\n",
    "            yi = y[i:(i+mb_size)]\n",
    "            \n",
    "            sum_grads = [np.zeros(shape=p.shape) for p in self.parameters]\n",
    "            losses = 0\n",
    "            \n",
    "            a1, h1, a2, h2, oa, os = self.forward(self,xi,yi)\n",
    "            grad = self.backward(self, xi, yi, a1, h1, a2, h2, oa, os, cache=none)\n",
    "            \n",
    "            sum_grads = [g1 + g2 for g1, g2 in zip(grad, sum_grads)]\n",
    "            loss = self.loss(self, yi, os)\n",
    "            losses += loss                          \n",
    "            average_grads = [g / xi.shape[0] for g in sum_grads]\n",
    "            average_loss = losses / xi.shape[0]\n",
    "            self.parameters = update_parms(average_grads, mu)\n",
    "        \n",
    "            return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(self):\n",
    "    .\n",
    "    ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Miniconda3]",
   "language": "python",
   "name": "conda-env-Miniconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
